---
title:    "Text Mining en Social Media: Analisis de Sentimiento en Twitter, Topic Covid"
author:   "by [María Luisa Duque](https://www.linkedin.com/in/marialuisaduque/)"
mail:     "marialdu@ucm.es"
linkedin: "marialuisaduque"
github:   "mlduque"
date:     "`r Sys.Date()`"
license:  by-nc-sa
urlcolor: blue
output:
  html_document: 
    theme:        cosmo # "default", "cerulean", "journal", "flatly", "readable", "spacelab", "united", "cosmo", "lumen", "paper", "sandstone", "simplex", "yeti"
    highlight:    tango # "default", "tango", "pygments", "kate", "monochrome", "espresso", "zenburn", "haddock", "textmate"
    toc:          TRUE
    toc_float:    TRUE
    code_folding: show
    includes:
      after_body: footer.html
  pdf_document:   default
  epuRate::epurate:
    number_sections: FALSE
    code_folding:    "show"
    toc:          TRUE 
    word_document:  default
  rmdformats::readthedown:
    toc:          TRUE 
    toc_float:    TRUE     
---
  


## Análisis de sentimiento con datos de Twitter

Instalamos los paquetes requeridos.

```{r, eval=FALSE}
# Si queremos ejecutar este bloque de código hay que cambiar EVAL=TRUE o eliminar EVAL=FALSE
# install.packages("Rstem")
install.packages("~/TextMining/Rstem_0.4-1.tar.gz", repos = NULL, type = "source")
# install.packages("sentiment", repos = "http://www.omegahat.org/R", type = "source")
require(devtools)
# install.packages("Rstem", repos = "http://www.omegahat.org/R", type = "source")
# install.packages("~/TextMining/sentiment_0.2.tar.gz", repos = NULL, type = "source")
install_url("http://cran.r-project.org/src/contrib/Archive/sentiment/sentiment_0.2.tar.gz")
require(sentiment)
ls("package:sentiment")
install.packages("tm")
install.packages("wordcloud")
```

Cargamos las librerías.

```{r, message=FALSE}
# library(twitteR)
library(rtweet)
library(wordcloud)
library(RColorBrewer)
library(plyr)
library(ggplot2)
library(sentiment) # Instalación local
library(sentimentr)
library(Rstem) # Instalación local
library(ggplot2)
library(gridExtra)
library(wordcloud)
library(igraph)
library(graphTweets)
library(tidyverse)
```

Antes de empezar hay que darse de alta en Twitter como desarrollador y registrar un App en el API: https://apps.twitter.com/

Con eso se pueden incluir los datos en este script o en otro fichero de extensión .R

* api_key             <- "********************"
* api_secret          <- "********************"
* access_token        <- "********************"
* access_token_secret <- "********************"
* twitter_app         <- "********************"

En nuestro caso los hemos incluido en el fichero twitterAuth3.R que está en nuestro directorio de trabajo (fichero no disponible o de acceso público, evidentemente, para poder replicar esta tarea deben lograr sus propios credenciales).

Cargamos el fichero y accedemos a Twitter.

```{r}
# Cargamos las credenciales
source('twitterAuth3.R')

# Accedemos a Titter a través de los datos del token
create_token(
  app             = twitter_app,
  consumer_key    = api_key,
  consumer_secret = api_secret,
  access_token    = access_token,
  access_secret   = access_token_secret)
```


### Recopilación de tweets

La librería **rtweet** nos ofrece algunas funciones para sacar información general de Twitter, como:

```{r}
# Los emojis
emojis
```

Los lenguajes aceptados en Twitter.

```{r}
langs
```

Hacemos una petición de 18000 tweets haciendo uso de la query "covid", la búsqueda q = "Covid" busca tweets que contengan "covid" ubicados en cualquier parte de los tweets y en cualquier orden. En español.

```{r, error=TRUE}
covid_tweets <- search_tweets("covid", n = 18000, lang="es", include_rts = FALSE)
```

Podemos guardarlos.

```{r, error=TRUE}
save(covid_tweets, file = './data/covid_tweets_2020-06-07.RData')
```

En vez de sacar los datos de Twitter, podemos cargar el fichero.

```{r}
load("./data/covid_tweets_2020-06-07.RData")
```

Vemos el tamaño del objeto.

```{r}
dim(covid_tweets)
```
### Análsis de usuarios

Detalles de los usuarios que han producido los tweets que hemos capturado.

```{r}
users_data(covid_tweets) %>% head()
```

Usuarios más populares.

```{r, error=TRUE}
options(scipen = 20)
ggplot(covid_tweets) +
    geom_histogram(aes(x = followers_count))
```

Top 5 de los usuarios más populares (con más seguidores), su procedencia y el contenido del tweet.

```{r, error=TRUE}
covid_tweets %>% 
    top_n(5, followers_count) %>% 
    arrange(desc(followers_count)) %>% 
    select(screen_name, followers_count, location, text)
```


### Tweets más populares

En base a la cantidad de retweets que recibieron. Nos quedamos sólo con los tweets originales, descartando los que son retweets en sí mismos (“is_retweet == TRUE”) y revisamos la distribución de sus retweets.

```{r, error=TRUE}
ggplot(filter(covid_tweets, !is_retweet))+
    geom_histogram(aes(x = retweet_count))
```

Tweet original que sumó más retweets.

```{r, error=TRUE}
covid_tweets %>% 
    filter(!is_retweet) %>% 
    filter(retweet_count == max(retweet_count)) %>% 
    select(screen_name, retweet_count, followers_count, location, text)
```

### Hora y día en la que se producen los tweets

*rtweet()* provee de una función que hace fácil mostrar la evolución temporal de nuestros tweets: ts_plot(). Podemos ver la frecuencia de tweets por segundo, hora, día, semana, mes o año eligiendo el parámetro correspondiente (“seconds”, “minutes”, “hours”, “days”, “weeks”, “months” o “years”).

```{r, error=TRUE}
ts_plot(covid_tweets, "minutes")
```

### Procedencia de los usuarios

Vamos a extraer el top 10 de los lugares más frecuentes, eliminando los tweets de usuarios sin datos en su atributo *location*.

```{r, error=TRUE}
covid_tweets %>%
    filter(location != "", !is.na(location)) %>% 
    count(location) %>% 
    top_n(10, n) %>% 
    ggplot() +
      geom_col(aes(x = reorder(location, n), y = n)) + 
      coord_flip() +
      labs(title = "Procedencia de los usuarios",
           x = "ubicación",
           y = "cantidad")
```


### Tweets en tiempo real

La función **stream_tweets()** permite iniciar una conexión y capturar tweets hasta que concluya el tiempo dispuesto por el parámetro *timeout*, expresado en segundos. Vamos a *escuchar* el stream de Twitter durante dos minutos (120 segundos), y capturar mensajes que incluyan la palabra *covid* en este caso.

```{r, error=TRUE}
captura_streaming <- stream_tweets(q = "covid", timeout = 120, language = "es")
captura_streaming[4:5]
```

### Emojis

Vamos a llevar un pequeño análisis/exploración sobre los emojis.

Instalamos el paquete requerido.

```{r, eval=FALSE}
devtools::install_github("GuangchuangYu/emojifont")
```

Cargamos la librería y buscamos el emoji deseado.

```{r}
library(emojifont)
search_emoji('smile')
```

```{r}
emoji(search_emoji('smile'))
```

Para poder generar un gráfico de los emojis requerimos cargar emoji font.

Lista disponible de emoji fonts.

```{r}
list.emojifonts()
```

Cargamos el emoji font seleccionado.

```{r}
load.emojifont('OpenSansEmoji.ttf')
```

Gráfico.

```{r}
set.seed(42)
x <- rnorm(10)
set.seed(321)
y <- rnorm(10)
plot(x, y, cex=0)
text(x, y, labels=emoji('cow'), cex=1.5, col='steelblue', family='OpenSansEmoji')
```


Gráfico con ggplo2.

```{r}
d <- data.frame(x=x, y=y,
     label = sample(c(emoji('cow'), emoji('camel')), 10, replace=TRUE),
     type = sample(LETTERS[1:3], 10, replace=TRUE))
require(ggplot2)
ggplot(d, aes(x, y, color=type, label=label)) + 
    geom_text(family="OpenSansEmoji", size=5)
```

Gráfico más útil con ggplot2.

```{r}
dd=data.frame(x=emoji(c("satisfied", "disapointed")), y=c(50, 10))
emoji_text=element_text(family="OpenSansEmoji", size=20)
ggplot(dd, aes(x, y)) + geom_bar(stat='identity', aes(fill=x)) +
     ggtitle(paste(emoji(c("+1", "-1")), collapse=" "))+
     theme(axis.text.x = emoji_text, legend.text=emoji_text, title=emoji_text) +
     xlab(NULL)+ylab(NULL)
```


### ETL: Extract, transformation and load

Sacamos el texto.

```{r}
covid_txt <- covid_tweets$text
head(covid_txt, 10)
```

Quitamos los retweets.

```{r}
covid_txt <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", covid_txt)
head(covid_txt, 10)
```

Podemos quitar las personas (@), los usuarios, con el siguiente código.

```{r, eval=FALSE}
covid_txt <- gsub("@\\w+", "", covid_txt)
head(covid_txt, 10)
```

Quitamos los html, links.

```{r}
covid_txt <-  gsub("\\bhttp[a-zA-Z0-9]*\\b", "", covid_txt)
head(covid_txt, 10)
```

Puntuación.

```{r}
covid_txt <- gsub("[[:punct:]]", "", covid_txt)
head(covid_txt, 10)
```

Podríamos quitar palabras por nosotros elegidas del siguiente modo. Quitamos la palabra amp, en este caso.

```{r, eval=FALSE}
covid_txt <- gsub("amp", "", covid_txt)
head(covid_txt, 10)
```

Caracteres no alfanuméricos.

```{r}
covid_txt <- gsub("[^a-zA-Z0-9 ]", "", covid_txt)
head(covid_txt, 10)
```

Quitamos los tco. Los tco son un servicio que se usan para cortar los links. Direcciones cortas.

```{r}
covid_txt <-  gsub("\\btco[a-zA-Z0-9]*\\b", "", covid_txt)
head(covid_txt, 10)
```

Quitamos NAs.

```{r}
covid_txt <- covid_txt[!is.na(covid_txt)]
head(covid_txt, 10)
```

Podríamos quitar los emoticonos. No vamos a ejecutar este código (eval=FALSE).

```{r, eval=FALSE}
covid_txt <- iconv(covid_txt, 'UTF-8', 'ASCII')
head(covid_txt, 10)
```

Pasamos a minúsculas.

```{r}
covid_txt <- tolower(covid_txt)
head(covid_txt, 10)
```

Espacios, tabulaciones.

```{r}
covid_txt <- gsub("[ \t]{2,}", "", covid_txt)
covid_txt <- gsub("^\\s+|\\s+$", "", covid_txt)
head(covid_txt, 10)
```


Vemos como nos ha quedado el texto.

El original.

```{r}
head(covid_tweets$text, 10)
```

Y el limpio.

```{r}
head(covid_txt, 10)
```

La diferencia, tras la limpieza, es sustancial.

### Clasificación de emociones

Empezamos clasificando emociones con *classify_emotion* usando un algoritmo de Bayes.


```{r, error=TRUE}
covid_class_emo <- classify_emotion(covid_txt, algorithm="bayes", prior=1.0)
```

La función nos retorna 7 columnas: anger, disgust, fear, joy, sadness, surprise y best_fit para cada fila del documento

```{r, error=TRUE}
head(covid_class_emo)
```

Vamos a tratar de hacer un algoritmo de clasificación. 
La que según el algoritmo encaja mejor (BEST_FIT) la guardamos.
Si bien, en ocasiones, no puede clasificar bien si la mayoría son NAs en el BEST_FIT.

```{r}
# Guardamos en el objeto emotion
emotion <- covid_class_emo[, 7]

# Y lo vemos en una tabla
table(emotion, useNA = 'ifany')
```

Para facilitar la clasificación, vamos a substituir los NAs por unknown.

```{r}
emotion[is.na(emotion)] <- "unknown"
table(emotion, useNA = 'ifany')
```

En un gráfico.

```{r, fig.align="center"}
pie <- ggplot(as.data.frame(covid_class_emo), aes(x = factor(1), fill = factor(BEST_FIT))) +
 geom_bar(width = 1)
pie + coord_polar(theta = "y") + labs(title = 'Sentimiento Covid')
```


Ahora lo vemos quitando los NAs.

```{r}
pie <- ggplot(as.data.frame(covid_class_emo), aes(x = factor(1), fill = factor(BEST_FIT)))  + geom_bar(width = 1)

pie + coord_polar(theta = "y") + ggtitle("Sentimiento Covid") + ylab("Y") + xlab("X") + scale_fill_brewer(palette = "RdYlGn") + theme(plot.title = element_text(size=12, face='bold'))
```


Barras.

```{r, fig.align="center"}
g <- ggplot(as.data.frame(covid_class_emo), aes(x = BEST_FIT)) +
 geom_bar() + labs(title = 'Sentimiento Covid') + geom_bar(aes(y=..count.., fill=emotion)) +
      scale_fill_brewer(palette="Dark2")
g 
```

Obtenemos la **polaridad**. Clasificamos el texto en cuatro categorías. 

* POS:      Sentimiento positivo
* NEG:      Sentimiento negativo
* POS/NEG:  No definido
* BEST_FIT: La mas probable

```{r}
covid_class_pol <- classify_polarity(covid_txt, algorithm="bayes")
head(covid_class_pol, 3)
```

Como antes, creamos un objeto con el resultado, en este caso la polaridad.

```{r}
polarity <- covid_class_pol[, 4]
head(polarity)
```

En una tabla.

```{r}
table(polarity, useNA = 'ifany')
```

Recopilamos la información en un dataframe.

```{r}
sentiment_dataframe <- data.frame(text     = covid_txt, 
                                  emotion  = emotion, 
                                  polarity = polarity, stringsAsFactors=FALSE)
#head(sentiment_dataframe)
```

Lo reordenamos (decreciente).

```{r}
sentiment_dataframe <- within(sentiment_dataframe, emotion <- factor(emotion, levels=names(sort(table(emotion), decreasing=TRUE))))
#head(sentiment_dataframe)
```

Nombres de las variables.

```{r}
names(sentiment_dataframe)
```

Estructura del dataframe.

```{r}
str(sentiment_dataframe)
```

Resumen del dataframe.

```{r}
summary(sentiment_dataframe)
```


### Gráficos

Número de Tweets clasificados por categorías.

```{r, fig.align="center"}
ggplot(sentiment_dataframe, aes(x = emotion)) + geom_bar(aes(y = ..count.., fill = emotion)) +
      scale_fill_brewer(palette = "Dark2") +
      ggtitle("Análisis de sentimiento Covid en Twitter") +
      theme(legend.position="right") + ylab("Número de Tweets") + xlab("Tipos de emoción")
```

Gráfico de Polaridad por tweets.

```{r, fig.align="center"}
ggplot(sentiment_dataframe, aes(x = polarity)) +
      geom_bar(aes(y = ..count.., fill = polarity)) +
      scale_fill_brewer(palette = "RdGy") +
      ggtitle("Análisis de sentimiento Covid en Twitter") +
      theme(legend.position="right") + ylab("Número de Tweets") + xlab("Tipos de polaridad")
```

Separamos las palabras según las emociones.

```{r}
covid_emos     <- levels(factor(sentiment_dataframe$emotion))
n_covid_emos   <- length(covid_emos)
covid_emo_docs <- rep("", n_covid_emos)
for (i in 1:n_covid_emos)
{
      tmp <- covid_txt[emotion == covid_emos[i]]
      covid_emo_docs[i] <- paste(tmp, collapse=" ")
}
```

Las vemos.

```{r}
head(covid_emos)
```

### Wordclouds: Nubes de palabras


Finalmente, veamos las palabras en los tweets y creamos una nube de palabras que usa las emociones de las palabras para determinar su ubicación dentro de la nube.


```{r, warning=FALSE, message=FALSE}
covid_sentiment_df = data.frame(text=covid_txt, emotion=emotion,
polarity=polarity, stringsAsFactors=FALSE)

# Separamos el texto segun las emociones
emotion_covid = levels(factor(covid_sentiment_df$emotion))
emotion_length = length(emotion_covid)
emotion_covid.docs = rep('', emotion_length)
for (i in 1: emotion_length) 
{
tmp = covid_txt[emotion == emotion_covid[i]]
emotion_covid.docs[i] = paste(tmp, collapse=' ')
}
 
# Eliminamos las Stopwords
emotion_covid.docs = removeWords(emotion_covid.docs, stopwords('es'))

# Creamos el corpus
corpus = Corpus(VectorSource(emotion_covid.docs))
tdm = TermDocumentMatrix(corpus)
tdm = as.matrix(tdm)
colnames(tdm) = emotion_covid
 
# Dibujamos una Comparison Wordcloud
comparison.cloud(tdm, colors = brewer.pal(emotion_length, 'Dark2'),
scale = c(1,.2), max.words = 300,random.order = FALSE, title.size = 1.5)
```


La presente nube de palabras no parece verse correctamente al ser renderizada. Vamos a mostrarla una vez salvada la imagen.

![WordCloud1](C:/Users/maria/Desktop/Temario Big Data/12 Text Mining/TextMining/word1.jpg)



#### Corpus

Creamos un corpus.

```{r}
covid_corpus <- Corpus(VectorSource(covid_txt))
inspect(covid_corpus[1:10])
```

Lo limpiamos con *tm_map*.

```{r}
# Pasamos a minúsculas
corpus_clean <- tm_map(covid_corpus, tolower)

# Quitamos los números
corpus_clean <- tm_map(corpus_clean, removeNumbers)

# Quitamos puntauación
corpus_clean <- tm_map(corpus_clean, removePunctuation)

# Quitamos stopwords
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords("es"))

# Quitamos espacios en blanco
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
```

Generamos un dtm (Document-Term Matrix). Para ver cuántas veces aparecen las palabras en el documento.

```{r}
covid_dtm <- DocumentTermMatrix(covid_corpus, control = list(minWordLength = 1, 
                                                       stopwords = TRUE))
inspect(covid_dtm)
```

Hacemos el stem.

```{r, warning=FALSE}
covid_corpus_stem <- tm_map(covid_corpus, stemDocument)
covid_corpus_stem <- tm_map(covid_corpus_stem, stemCompletion, dictionary = covid_corpus)
inspect(covid_corpus_stem[1:5])
```

Observamos los términos más frecuentes. Ponemos la frecuencia mínima de 10, en un “head” que nos devuelva los 40 primeros.

```{r}
head(findFreqTerms(covid_dtm, lowfreq=10), 40)
```



**Segunda wordcloud: Nube de palabras.**

Generamos una segunda nube de palabras más genérica.

```{r, fig.align="center", warning=FALSE}
wordcloud(corpus_clean, min.freq = 30, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
```

La presente nube de palabras no parece verse correctamente al ser renderizada. Vamos a mostrarla una vez salvada la imagen.

![WordCloud2](C:/Users/maria/Desktop/Temario Big Data/12 Text Mining/TextMining/word2.jpg)


### Obtención de las tendencias

Para un momento determinado, el actual en el momento de ejecución del presente código, las tendencias disponibles serían.

```{r, eval=TRUE, error=TRUE}
# Del paquete twitteR
# availableTrendLocations()

# Con el paquete rtweet
get_trends()
```

Y también las podemos obtener por localización, en este caso se usa el ID WOEID.

```{r, error=TRUE}
# Del paquete rtweet
tendencias_locales <- trends_available()
head(tendencias_locales)
```

Vemos la tabla por países.

```{r, error=TRUE}
table(tendencias_locales$countryCode)
```

Para España.

```{r, error=TRUE}
tendencias_locales[tendencias_locales$countryCode=='ES', ]
```

Vemos que España es el woeid 23424950, vemos las tendencias en España.

```{r, error=TRUE}
get_trends(woeid = '23424950')
```

Terminamos ofreciendo la información de la sesión.

```{r}
sessionInfo()
```


### Referencias, Comentarios y Links

**Referencias**

Parte de este código ha sido recopilado de las lecciones del profesor [Santiago Mota](https://www.linkedin.com/in/santiagomota/), Master in Big Data and & Data Science at Complutense University of Madrid, que a su vez ha sido inspirado de la documentación [Join, split, and compress PDF files with pdftools](https://www.r-bloggers.com/join-split-and-compress-pdf-files-with-pdftools/).

**Links**

* Ejemplo de carga de fichero y acceso a Twitter, [aqui.](https://colinpriest.com/2015/07/04/tutorial-using-r-and-twitter-to-analyse-consumer-sentiment/)
* Ejemplo conectando R a Twitter [aqui.](https://rpubs.com/HAVB/rtweet)
* Información sobre exploración de emojis [aqui,](https://yulab-smu.github.io/treedata-book/chapter8.html#phylomoji) o vía CRAN [aqui.](https://cran.rstudio.com/web/packages/emojifont/index.html)

**Comentarios**

El presente código ha sido ejecutado en el sistema operativo version Windows 10, 64 bits, escogiendo formato de codificación en RStudio UTF-8.

